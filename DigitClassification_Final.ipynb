{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Sameer Kumar\n",
    "Date: 24/10/2018\n",
    "'''\n",
    "\n",
    "'''\n",
    "This file trains Neural Network for Digit Classifications of MNIST Using BackPropagation\n",
    "Neural Architecture:\n",
    "1 - Three Layers - 1: Input Layer 2: Hidden Layer 3: Output Layer\n",
    "Input Layer - 784 Inputs\n",
    "Hidden Layer - 40 Neurons Activation Function tanh\n",
    "Output Layer - 10 Neurons Activation Function softmax\n",
    "2 - Weight: w(i, j) indicates weight fed to ith neuron from jth input\n",
    "Weights in Layer 2 (Hidden Layer) W_Layer_2 [ w(1,0).....w(1,784)\n",
    "                                              ...................\n",
    "                                              w(40,0).....w(40,784)] 40x785\n",
    "Weights in Layer 3 (Output Layer) W_Layer_3 [ w(1,0).....w(1,40)\n",
    "                                              ...................\n",
    "                                              w(10,0).....w(10,40)] 10x41\n",
    "3 - Energy or Loss function: Cross Entropy \n",
    "4 - Weights have bias\n",
    "Note: This network is designed to have only have two layers but can have user defined neurons in each layer.\n",
    "Weights in Layer 2: n x (785), 784 input neurons, +1 to that to incorporate bias\n",
    "Weights in Layer3: 10 x (n+1), 10 output neurons works well with Softmax, +1 to incorporate bias.\n",
    "'''\n",
    "\n",
    "'''\n",
    "This code was designed to use following Heuristics to make it efficient:\n",
    "1 - Scaled/Normalized the Training Set to have mean 0 and variance 1. The parameters used to normalize the Training Set\n",
    "was used to normalize Test Set.\n",
    "Note: To report accuracy on different Test Set that data must be normalized in the same way as above\n",
    "2 - Momentum Method was incorporated in update step.\n",
    "Note: This code has regularization has incorporated in it. But Hyperparameter search is highly necessary. So set it to zero.\n",
    "3 - In last layer Softmax activation function coupled with Cross Entropy loss function was used.\n",
    "4 - Before every epoch of training Shuffling of Training Set Elements was done.\n",
    "5 - Weights were initialized by taking inspiration from Xavier initialisation. I didn't follow it as it is, but modified to get:\n",
    "    Layer 1 weights: Gaussian Random Variable N(0, sqrt(2/784))\n",
    "    Layer 2 weights: Gaussian Random Variable N(0, sqrt(2/10))\n",
    "    Imp: Even if you change the number of neurons in the hidden layer don't change the weight how weights are initialised.\n",
    "Note: Code requires optimization to decrease execution time. \n",
    "Note: Set alpha = 0, beta = 0.9, eta = 5\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Files\n",
    "import numpy\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from LoadData import load_training_labels, load_training_images\n",
    "from LoadData import load_test_images, load_test_labels\n",
    "from LoadData import InitialWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "scaler = preprocessing.StandardScaler().fit(load_training_images('train-images-idx3-ubyte.gz'))\n",
    "Images = scaler.transform(load_training_images('train-images-idx3-ubyte.gz'))\n",
    "Labels_Train = load_training_labels('train-labels-idx1-ubyte.gz')\n",
    "Test = scaler.transform(load_test_images('t10k-images-idx3-ubyte.gz'))\n",
    "Labels_Test = load_test_labels('t10k-labels-idx1-ubyte.gz')\n",
    "if numpy.DataSource().exists('InitialWeightsLayer2.txt') and numpy.DataSource().exists('InitialWeightsLayer3.txt'):\n",
    "    W_Layer_2_Guess = numpy.loadtxt('InitialWeightsLayer2.txt')  # Load the Initial Weights\n",
    "    W_Layer_3_Guess = numpy.loadtxt('InitialWeightsLayer3.txt')   # Load the Initial Weights\n",
    "else:\n",
    "    W_Layer_2_Guess, W_Layer_3_Guess = InitialWeights()\n",
    "    numpy.savetxt('InitialWeightsLayer2.txt', W_Layer_2_Guess)  # Generate the Weights and save them\n",
    "    numpy.savetxt('InitialWeightsLayer3.txt', W_Layer_3_Guess)  # Generate the Weights and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0  # Regularization Parameter\n",
    "beta = 0.9  # Momentum Parameter\n",
    "eta = 5  # Learning Rate\n",
    "\n",
    "# Parameters\n",
    "iterations = 0  # No. of training epochs\n",
    "epsilon = 0.047  # Error Ratio = Wrong_Predictions/Total_Sample_Size\n",
    "M21 = 0  # Momentum Vector for Layer 2 Bias\n",
    "M22 = 0  # Momentum Vector for Layer 2 Weights\n",
    "M31 = 0  # Momentum Vector for Layer 3 Bias\n",
    "M32 = 0  # Momentum Vector for Layer 3 Weights\n",
    "Epoch = numpy.array([])  # Array for storing No. of Training Iterations\n",
    "CE_Train = numpy.array([])  # Mean Squared Error on Training Set\n",
    "Error_Train = numpy.array([])  # No. of Misclassfications on Training Set\n",
    "CE_Test = numpy.array([])  # Mean Squared Error on Test Set\n",
    "Error_Test = numpy.array([])  # No. of Misclassfications on Test Set\n",
    "max_iter = 500  # Maximum allowed Iterations for convergence\n",
    "row, col = Images.shape  # Shape of Input 60000x784\n",
    "row1, col1 = Test.shape  # Shape of Input 10000x784\n",
    "D_Train = numpy.zeros((row, 10))\n",
    "D_Train[numpy.arange(row), Labels_Train] = 1 # Desired Output based on correct Labels for Training Set\n",
    "D_Test = numpy.zeros((row1, 10))\n",
    "D_Test[numpy.arange(row1), Labels_Test] = 1 # Desired Output based on correct Labels for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Functions\n",
    "# SoftMax\n",
    "def softmax(v):\n",
    "    e = numpy.exp(v - numpy.max(v))\n",
    "    return e/numpy.sum(e)\n",
    "\n",
    "\n",
    "# Forward Pass\n",
    "def forward_pass(image, w_layer_2, w_layer_3):\n",
    "    # V2 and V3 are Locally Induced Fields at Layer 2 and 3\n",
    "    # Y2 and Y3 are Locally Induced Fields at Layer 2 and 3\n",
    "    temp_x = numpy.concatenate(([1], image), axis=0)\n",
    "    v2 = numpy.dot(w_layer_2, temp_x.T)\n",
    "    y2 = numpy.tanh(v2)\n",
    "    temp_y = numpy.concatenate(([1], y2), axis=0)\n",
    "    v3 = numpy.dot(w_layer_3, temp_y)\n",
    "    y3 = softmax(v3)\n",
    "    return v2, v3, y2, y3\n",
    "\n",
    "\n",
    "# Backward Pass\n",
    "def backward_pass(image, d, w_layer_3, v2, y2, y3):\n",
    "    delta3 = (1/row) * (d - y3)\n",
    "    derivative_layer2 = numpy.array([1 - numpy.square(numpy.tanh(i)) for i in v2])\n",
    "    delta2 = numpy.multiply(numpy.dot(w_layer_3[:, 1:].T, delta3), derivative_layer2)\n",
    "    gradient_layer_2 = numpy.matmul(-delta2[:, numpy.newaxis], numpy.concatenate(([1], image), axis=0)[:, numpy.newaxis].T)\n",
    "    gradient_layer_3 = numpy.matmul(-delta3[:, numpy.newaxis], numpy.concatenate(([1], y2), axis=0)[:, numpy.newaxis].T)\n",
    "    return gradient_layer_2, gradient_layer_3\n",
    "\n",
    "\n",
    "# Update Weights\n",
    "def update_weights(m21, m22, m31, m32, gradient_layer_2, gradient_layer_3, w_layer_2, w_layer_3):\n",
    "    m21 = (beta * m21) - (eta * (gradient_layer_2[:, 0]))  # Momentum - Bias Layer 2\n",
    "    m22 = (beta * m22) - (eta * (gradient_layer_2[:, 1:] + alpha/60000 * w_layer_2[:, 1:]))  # Momentum plus Regularization - Weights Layer 2\n",
    "    m31 = (beta * m31) - (eta * (gradient_layer_3[:, 0]))  # Momentum - Bias Layer 3\n",
    "    m32 = (beta * m32) - (eta * (gradient_layer_3[:, 1:] + alpha/60000 * w_layer_3[:, 1:]))  # Momentum plus Regularization - Weights Layer 3\n",
    "    w_layer_2[:, 0] = w_layer_2[:, 0] + m21\n",
    "    w_layer_2[:, 1:] = w_layer_2[:, 1:] + m22\n",
    "    w_layer_3[:, 0] = w_layer_3[:, 0] + m31\n",
    "    w_layer_3[:, 1:] = w_layer_3[:, 1:] + m32\n",
    "    return w_layer_2, w_layer_3, m21, m22, m31, m32\n",
    "\n",
    "\n",
    "# Calculate Cross Entropy Error and Misclassifications\n",
    "def calculate_ce(image, label, w_layer_2, w_layer_3):\n",
    "    rows, cols = image.shape\n",
    "    ce = 0\n",
    "    error = 0\n",
    "    for i in range(0, rows):\n",
    "        v2, v3, y2, y3 = forward_pass(image[i], w_layer_2, w_layer_3)\n",
    "        d = numpy.zeros(10)\n",
    "        d[label[i]] = 1\n",
    "        ce = ce + numpy.sum(numpy.dot(-d, numpy.log(y3.T)))/rows + alpha * (numpy.sum(numpy.square(w_layer_3[:, 1:]))/rows)\n",
    "        if label[i] != numpy.argmax(y3):\n",
    "            error += 1\n",
    "    return ce, error\n",
    "\n",
    "# Learning Rate Decay\n",
    "def check_learning_rate(eta_prime, ce):\n",
    "    if ce[-1] >= ce[-2]:\n",
    "        eta_prime = 0.4 * eta_prime\n",
    "    return eta_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Loop\n",
    "# Iteration 0\n",
    "# Backpropagation\n",
    "L = numpy.arange(60000)\n",
    "numpy.random.shuffle(L) # Shuffling\n",
    "temp_w2 = W_Layer_2_Guess\n",
    "temp_w3 = W_Layer_3_Guess\n",
    "for i in L:\n",
    "    V2, V3, Y2, Y3 = forward_pass(Images[i], temp_w2, temp_w3)\n",
    "    Gradient_Layer_2, Gradient_Layer_3 = backward_pass(Images[i], D_Train[i], temp_w3, V2, Y2, Y3)\n",
    "    temp_w2, temp_w3, M21, M22, M31, M32 = update_weights(M21, M22, M31, M32, Gradient_Layer_2, Gradient_Layer_3, temp_w2, temp_w3)\n",
    "# Book Keeping\n",
    "Epoch = numpy.concatenate((Epoch, [iterations]), axis=0)\n",
    "ce_train, e_train = calculate_ce(Images, Labels_Train, temp_w2, temp_w3)\n",
    "CE_Train = numpy.concatenate((CE_Train, [ce_train]), axis=0)\n",
    "Error_Train = numpy.concatenate((Error_Train, [e_train]), axis=0)\n",
    "ce_test, e_test = calculate_ce(Test, Labels_Test, temp_w2, temp_w3)\n",
    "CE_Test = numpy.concatenate((CE_Test, [ce_test]), axis=0)\n",
    "Error_Test = numpy.concatenate((Error_Test, [e_test]), axis=0)\n",
    "# Print\n",
    "print('Epoch: ', iterations, ' CE on Training Set: ', ce_train, ' Error on Training Set: ', e_train, ' Error on Test Set: ', e_test, '\\n')\n",
    "# Next...\n",
    "iterations += 1\n",
    "# Remaining Epochs\n",
    "while iterations <= max_iter:\n",
    "    # Backpropagation\n",
    "    L = numpy.arange(60000)\n",
    "    numpy.random.shuffle(L) # Shuffling\n",
    "    for i in L:\n",
    "        V2, V3, Y2, Y3 = forward_pass(Images[i], temp_w2, temp_w3)\n",
    "        Gradient_Layer_2, Gradient_Layer_3 = backward_pass(Images[i], D_Train[i], temp_w3, V2, Y2, Y3)\n",
    "        temp_w2, temp_w3, M21, M22, M31, M32 = update_weights(M21, M22, M31, M32, Gradient_Layer_2, Gradient_Layer_3, temp_w2, temp_w3)\n",
    "    # Book Keeping\n",
    "    Epoch = numpy.concatenate((Epoch, [iterations]), axis=0)\n",
    "    ce_train, e_train = calculate_ce(Images, Labels_Train, temp_w2, temp_w3)\n",
    "    CE_Train = numpy.concatenate((CE_Train, [ce_train]), axis=0)\n",
    "    Error_Train = numpy.concatenate((Error_Train, [e_train]), axis=0)\n",
    "    ce_test, e_test = calculate_ce(Test, Labels_Test, temp_w2, temp_w3)\n",
    "    CE_Test = numpy.concatenate((CE_Test, [ce_test]), axis=0)\n",
    "    Error_Test = numpy.concatenate((Error_Test, [e_test]), axis=0)\n",
    "    # Print\n",
    "    print('Epoch: ', iterations, ' CE on Training Set: ', ce_train, ' Error on Training Set: ', e_train, ' Error on Test Set: ', e_test, '\\n')\n",
    "    # Check Termination\n",
    "    if (Error_Test[-1]/10000) < epsilon:\n",
    "        # Save Final Weights\n",
    "        numpy.savetxt('FinalOptimalWeights2.txt', temp_w2)\n",
    "        numpy.savetxt('FinalOptimalWeights3.txt', temp_w3)\n",
    "        print('Optimal Weights Reached!!!!!')\n",
    "        break\n",
    "    else:\n",
    "        # Check Learning Rate\n",
    "        eta = check_learning_rate(eta, CE_Train)\n",
    "        # Next...\n",
    "        iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "# Plot 1\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.plot(Epoch, Error_Train, label='Training Set')\n",
    "ax1.plot(Epoch, Error_Test, 'g--', label='Test Set')\n",
    "plt.title(r'No. of Training Iterations VS No. of Misclassifications')\n",
    "plt.xlabel(r'Epoch $\\rightarrow$')\n",
    "plt.ylabel(r'Misclassifications $\\rightarrow$')\n",
    "plt.legend()\n",
    "plt.savefig('1.pdf')\n",
    "# Plot 2\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.plot(Epoch, CE_Train, label='Training Set')\n",
    "ax2.plot(Epoch, CE_Test, 'g--', label='Test Set')\n",
    "plt.title('No of Training Iterations VS Cross Entropy (CE)')\n",
    "plt.xlabel(r'Epoch $\\rightarrow$')\n",
    "plt.ylabel(r'CE $\\rightarrow$')\n",
    "plt.legend()\n",
    "plt.savefig('2.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
